{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Uploading Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEruPEIZfKQ8"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Installing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHcK7VvyfODm"
      },
      "outputs": [],
      "source": [
        "!pip install -q catboost shap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Importing necessary modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shap\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_percentage_error, r2_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Building models and ensembling them randomly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpfapXc7fPXM"
      },
      "outputs": [],
      "source": [
        "# === Load Data ===\n",
        "df_train = pd.read_csv(\"train.csv\")\n",
        "df_test = pd.read_csv(\"test.csv\")\n",
        "\n",
        "features = df_train.columns[:55].tolist()\n",
        "targets = df_train.columns[55:].tolist()\n",
        "\n",
        "X_train_raw = df_train[features]\n",
        "X_test_raw = df_test[features].iloc[:500]\n",
        "Y_train = df_train[targets]\n",
        "\n",
        "# === Impute + Scale ===\n",
        "imp = SimpleImputer()\n",
        "X_train = imp.fit_transform(X_train_raw)\n",
        "X_test = imp.transform(X_test_raw)\n",
        "\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "df_train_scaled = pd.DataFrame(X_train, columns=features)\n",
        "df_test_scaled = pd.DataFrame(X_test, columns=features)\n",
        "\n",
        "# === Tuned Parameters per BlendProperty ===\n",
        "param_grid = {\n",
        "    'BlendProperty1': {'depth': 6, 'learning_rate': 0.05, 'iterations': 2700, 'l2_leaf_reg': 4},\n",
        "    'BlendProperty2': {'depth': 5, 'learning_rate': 0.06, 'iterations': 2600, 'l2_leaf_reg': 3},\n",
        "    'BlendProperty3': {'depth': 6, 'learning_rate': 0.045, 'iterations': 3000, 'l2_leaf_reg': 5},\n",
        "    'BlendProperty4': {'depth': 4, 'learning_rate': 0.05, 'iterations': 2500, 'l2_leaf_reg': 4},\n",
        "    'BlendProperty5': {'depth': 7, 'learning_rate': 0.035, 'iterations': 2800, 'l2_leaf_reg': 6},\n",
        "    'BlendProperty6': {'depth': 6, 'learning_rate': 0.04, 'iterations': 2900, 'l2_leaf_reg': 5},\n",
        "    'BlendProperty7': {'depth': 5, 'learning_rate': 0.06, 'iterations': 2500, 'l2_leaf_reg': 3},\n",
        "    'BlendProperty8': {'depth': 6, 'learning_rate': 0.045, 'iterations': 2700, 'l2_leaf_reg': 4},\n",
        "    'BlendProperty9': {'depth': 5, 'learning_rate': 0.05, 'iterations': 2600, 'l2_leaf_reg': 4},\n",
        "    'BlendProperty10': {'depth': 6, 'learning_rate': 0.04, 'iterations': 3000, 'l2_leaf_reg': 5},\n",
        "}\n",
        "\n",
        "\n",
        "final_preds = []\n",
        "metrics_log = []\n",
        "\n",
        "for col in targets:\n",
        "    print(f\"\\nüîÅ Processing {col}\")\n",
        "    y_col = Y_train[col].values\n",
        "    p = param_grid[col]\n",
        "\n",
        "       # === Feature Selection using a base CatBoost model ===\n",
        "    base_model = CatBoostRegressor(\n",
        "        iterations=p['iterations'],\n",
        "        learning_rate=p['learning_rate'],\n",
        "        depth=p['depth'],\n",
        "        l2_leaf_reg=p['l2_leaf_reg'],\n",
        "        bagging_temperature=0.5,\n",
        "        random_seed=42,\n",
        "        loss_function='RMSE',\n",
        "        od_type=\"Iter\",\n",
        "        od_wait=50,\n",
        "        verbose=0\n",
        "    )\n",
        "    base_model.fit(X_train, y_col)\n",
        "    importance_df = base_model.get_feature_importance(prettified=True)\n",
        "    important_feature_ids = importance_df[importance_df['Importances'] > 0.1]['Feature Id'].astype(int).tolist()\n",
        "    important_feature_names = [features[i] for i in important_feature_ids]\n",
        "\n",
        "    if not important_feature_names:\n",
        "        print(f\"‚ö† No important features for {col}. Using all features.\")\n",
        "        important_feature_names = features\n",
        "\n",
        "    X_train_sub = df_train_scaled[important_feature_names].values\n",
        "    X_test_sub = df_test_scaled[important_feature_names].values\n",
        "\n",
        "    # === Train 3 CatBoost models\n",
        "    cb_preds_test_all = []\n",
        "    cb_preds_train_all = []\n",
        "\n",
        "    for seed_offset in [0, 11, 27]:  # 3 different seeds/models\n",
        "        model = CatBoostRegressor(\n",
        "            iterations=p['iterations'],\n",
        "            learning_rate=p['learning_rate'],\n",
        "            depth=p['depth'],\n",
        "            l2_leaf_reg=p['l2_leaf_reg'],\n",
        "            bagging_temperature=0.5,\n",
        "            random_seed=42 + seed_offset,\n",
        "            loss_function='RMSE',\n",
        "            od_type=\"Iter\",\n",
        "            od_wait=50,\n",
        "            verbose=0\n",
        "        )\n",
        "        model.fit(X_train_sub, y_col)\n",
        "        cb_preds_test_all.append(model.predict(X_test_sub))\n",
        "        cb_preds_train_all.append(model.predict(X_train_sub))\n",
        "\n",
        "    # === Train ANN on same features\n",
        "    ann = Sequential([\n",
        "        Input(shape=(X_train_sub.shape[1],)),\n",
        "        Dense(256, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "        Dense(64, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.2),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    ann.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
        "    es = EarlyStopping(patience=25, restore_best_weights=True, verbose=0)\n",
        "\n",
        "    ann.fit(X_train_sub, y_col, validation_split=0.1, epochs=400, batch_size=64, verbose=0, callbacks=[es])\n",
        "\n",
        "    ann_pred_test = ann.predict(X_test_sub).flatten()\n",
        "    ann_pred_train = ann.predict(X_train_sub).flatten()\n",
        "\n",
        "    # === Final Ensemble\n",
        "    cb_mean_test = np.mean(cb_preds_test_all, axis=0)\n",
        "    cb_mean_train = np.mean(cb_preds_train_all, axis=0)\n",
        "\n",
        "    final_pred_test = 0.8 * cb_mean_test + 0.2 * ann_pred_test\n",
        "    final_pred_train = 0.8 * cb_mean_train + 0.2 * ann_pred_train\n",
        "\n",
        "    final_preds.append(final_pred_test)\n",
        "\n",
        "    # === Evaluation\n",
        "    r2 = r2_score(y_col, final_pred_train)\n",
        "    mape = mean_absolute_percentage_error(y_col, final_pred_train)\n",
        "    metrics_log.append((col, r2, mape))\n",
        "    print(f\"{col} ‚úÖ R¬≤: {r2:.4f}, MAPE: {mape:.4f}\")\n",
        "\n",
        "# === Save Final Predictions ===\n",
        "pred_df = pd.DataFrame(np.array(final_preds).T, columns=targets)\n",
        "pred_df.insert(0, \"ID\", range(1, 501))\n",
        "pred_df.to_csv(\"catboost_ann_ensemble_predictions.csv\", index=False)\n",
        "print(\"\\nüìÅ Saved: catboost_ann_ensemble_predictions.csv\")\n",
        "\n",
        "# === Summary ===\n",
        "print(\"\\nüìä Final Scores:\")\n",
        "for name, r2, mape in metrics_log:\n",
        "    print(f\"{name} - R¬≤: {r2:.4f}, MAPE: {mape:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
